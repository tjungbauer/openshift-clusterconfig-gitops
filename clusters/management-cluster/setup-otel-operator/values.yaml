---
otel: &channel-otel stable
otel-namespace: &otel-namespace openshift-opentelemetry-operator

######################################
# SUBCHART: helper-operator
# Operators that shall be installed.
######################################
helper-operator:
  operators:
    opentelemetry-product:
      enabled: true
      namespace:
        name: *otel-namespace
        create: true
      subscription:
        channel: *channel-otel
        approval: Automatic
        operatorName: opentelemetry-product
        source: redhat-operators
        sourceNamespace: openshift-marketplace
      operatorgroup:
        create: true
        notownnamespace: true

########################################
# SUBCHART: helper-status-checker
# Verify the status of a given operator.
########################################
helper-status-checker:
  enabled: true

  approver: false

  checks:
    - operatorName: opentelemetry-product
      namespace:
        name: *otel-namespace
      syncwave: 1

      serviceAccount:
        name: "status-checker-otel"

rh-build-of-opentelemetry:
  #########################################################################################
  # namespace ... disabled here, since we deployed it via Tempo already
  #########################################################################################
  namespace:
    name: tempostack
    create: false
  
  #########################################################################################
  # OPENTELEMETRY COLLECTOR - Production Configuration
  #########################################################################################
  collector:
    enabled: true
    name: otel-collector
    mode: deployment
    replicas: 3
    
    serviceAccount: otel-collector-sa
    managementState: managed
    
    resources:
      limits:
        cpu: 2
        memory: 4Gi
      requests:
        cpu: 500m
        memory: 1Gi
    
    nodeSelector:
      node-role.kubernetes.io/worker: ""
    
    tolerations:
      - key: node-role.kubernetes.io/infra
        operator: Exists
        effect: NoSchedule
    
    config:
      receivers:
        # OTLP receivers for traces, metrics, and logs
        otlp:
          protocols:
            grpc:
              endpoint: 0.0.0.0:4317
            http:
              endpoint: 0.0.0.0:4318
              cors:
                allowed_origins:
                  - "https://*.example.com"
        
        # Prometheus receiver for scraping metrics
        prometheus:
          config:
            scrape_configs:
              - job_name: 'otel-collector'
                scrape_interval: 30s
                static_configs:
                  - targets: ['0.0.0.0:8888']
        
        # Jaeger receiver (if migrating from Jaeger)
        jaeger:
          protocols:
            grpc:
              endpoint: 0.0.0.0:14250
            thrift_http:
              endpoint: 0.0.0.0:14268
      
      processors:
        # Batch processor for better throughput
        batch:
          timeout: 10s
          send_batch_size: 1024
          send_batch_max_size: 2048
        
        # Memory limiter to prevent OOM
        memory_limiter:
          check_interval: 1s
          limit_mib: 3500
          spike_limit_mib: 512
        
        # Resource processor to add cluster attributes
        resource:
          attributes:
            - key: cluster.name
              value: production-cluster
              action: insert
            - key: k8s.cluster.name
              value: production-cluster
              action: insert
            - key: deployment.environment
              value: production
              action: insert
        
        # K8s attributes processor
        k8sattributes:
          auth_type: "serviceAccount"
          passthrough: false
          extract:
            metadata:
              - k8s.namespace.name
              - k8s.deployment.name
              - k8s.pod.name
              - k8s.pod.uid
              - k8s.node.name
      
      exporters:
        # Export traces to Tempo
        otlp/tempo:
          endpoint: tempo-gateway.tempo.svc.cluster.local:4317
          tls:
            insecure: false
            ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        
        # Export metrics to Prometheus
        prometheusremotewrite:
          endpoint: http://prometheus-server.monitoring.svc.cluster.local:9090/api/v1/write
          resource_to_telemetry_conversion:
            enabled: true
        
        # Export logs to Loki
        loki:
          endpoint: http://loki-gateway.loki.svc.cluster.local:3100/loki/api/v1/push
          labels:
            resource:
              k8s.namespace.name: "namespace"
              k8s.pod.name: "pod"
              k8s.container.name: "container"
        
        # Debug exporter for troubleshooting
        debug:
          verbosity: basic
          sampling_initial: 5
          sampling_thereafter: 200
      
      service:
        pipelines:
          # Traces pipeline
          traces:
            receivers: [otlp, jaeger]
            processors: [memory_limiter, k8sattributes, resource, batch]
            exporters: [otlp/tempo]
          
          # Metrics pipeline
          metrics:
            receivers: [otlp, prometheus]
            processors: [memory_limiter, resource, batch]
            exporters: [prometheusremotewrite]
          
          # Logs pipeline
          logs:
            receivers: [otlp]
            processors: [memory_limiter, k8sattributes, resource, batch]
            exporters: [loki]
        
        telemetry:
          logs:
            level: info
          metrics:
            level: detailed
            address: 0.0.0.0:8888
  